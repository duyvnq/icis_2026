---
title: "Analysis Report"
format: html
---


```{r}
library(tidyverse)
library(bibliometrix)

data_reg <- read.csv("data/processed_data.csv") |>
  select(id:quartile) |>
  select(-(agr_bio:heal))

data_bib <- read.csv("data/all_1020.csv")
```

## Number of outputs per year by type of cooperation.

1990 is selected as a cutoff year as before this year, the number of outputs is minimal.
Number of outputs by bilateral cooporation always higher than multilateral cooporation, until 2019.

```{r}
data_reg |> 
  select(year, coop) |>
  na.omit() |> 
  filter(year > 1990) |> 
  ggplot(aes(x = year, fill = coop)) +
  geom_bar(position = "dodge") +
  theme_bw()
```

## Number of outputs per year by OA

Number of OA paper increased sharply after 2010, surpassed non-OA paper after 2017.

```{r}
data_reg |>
  select(year, OA) |> 
  na.omit() |>
  filter(year > 1990) |>
  group_by(year, OA) |>
  summarise(Count = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = Count, color = OA)) +
  geom_line(linewidth = 1) +
  theme_bw()
```

## Number of outputs per year by domains

Physical sciences domain lead the outputs, has been the one start first and still top leading.
Social Sciences fields are much more humble.

```{r}
data_reg |>
  filter(year > 1990) |>
  select(year, LS, SS, PS, HS) |>
  pivot_longer(cols = LS:HS, names_to = "Category", values_to = "Value") |>
  filter(Value == 1) |>
  group_by(year, Category) |>
  summarise(Count = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = Count, color = Category)) +
  geom_line(linewidth = 1) +
  theme_bw()
```

## Number of outputs per year by funder

### Sponsor Origins

Japan and Vietnam are top funders.

```{r}
data_reg |>
  filter(year > 1990) |>
  select(year, asian, eu, int, us, jap, vn) |>
  pivot_longer(cols = asian:vn, names_to = "Funder", values_to = "Value") |>
  filter(Value == 1) |>
  group_by(year, Funder) |>
  summarise(Count = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = Count, color = Funder)) +
  geom_line(linewidth = 1) +
  theme_bw()
```

### Sponsor types


```{r}
data_reg |>
  filter(year > 1990) |>
  select(year, ind, pub, uni) |>
  pivot_longer(cols = ind:uni, names_to = "Funder", values_to = "Value") |>
  filter(Value == 1) |>
  group_by(year, Funder) |>
  summarise(Count = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = Count, color = Funder)) +
  geom_line(linewidth = 1) +
  theme_bw()
```

## Number of outputs per year by quartile

```{r}
data_reg |>
  filter(year > 1990) |>
  na.omit() |>
  group_by(year, quartile) |>
  summarise(Count = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = Count, fill = quartile)) +
  geom_col(position = "stack") +
  theme_bw()
```

## Number of citations


```{r}
ggplot(data_reg, aes(x = log1p(cited))) +
  geom_histogram(bins = 21, color = "white") +
  theme_bw()

ggplot(data_reg, aes(x = n_funder, y = log1p(cited))) +
  geom_point(alpha = 0.3) +
  geom_jitter(alpha = 0.3) +
  theme_bw()
```

## Comparing citations:

```{r}
# By OA
wilcox.test(cited ~ OA, data = data_reg)

# By coop
wilcox.test(cited ~ coop, data = data_reg)

# By fund or not
wilcox.test(cited ~ fund, data = data_reg)

wilcox.test(cited ~ vn, data = data_reg)

# By quartile
kruskal.test(cited ~ quartile, data = data_reg)
pairwise.wilcox.test(data_reg$cited, data_reg$quartile,
                     p.adjust.method = "bonferroni")

# By number of funders origins
kruskal.test(cited ~ n_funder, data = data_reg)
pairwise.wilcox.test(data_reg$cited, data_reg$n_funder,
                     p.adjust.method = "bonferroni")

# By number of funders types
kruskal.test(cited ~ n_ftype, data = data_reg)
pairwise.wilcox.test(data_reg$cited, data_reg$n_ftype,
                     p.adjust.method = "bonferroni")
```

Experiment


```{r}
# Chi-square for OA rates
table_oa <- table(data_reg$coop, data_reg$OA)
chisq.test(table_oa)

# ANOVA/Kruskal for SJR
kruskal.test(sjr_score ~ coop, data = data_reg |> filter(year > 1990, !is.na(sjr_score)))
```

Regression analysis

```{r}
library(MASS)

# Prepare data
data_regre <- data_reg |> 
  mutate(
    OA = factor(OA, labels = c("Not OA", "OA")),
    coop = factor(coop, labels = c("Bilateral", "Multilateral")),
    quartile = factor(quartile, levels = c("Q4", "Q3", "Q2", "Q1")),
    fund = as.factor(fund),
    across(c(LS:HS, ind, pub, uni, jap, vn, n_funder, n_ftype), as.factor)
  ) |> 
  na.omit(cited, OA, coop, n_countries, year, quartile, LS, SS, PS, HS, ind, pub, uni, jap, vn, n_ftype) |> 
  mutate(
    old = 2025 - year
  )

# Fit negative binomial regression model
nb_model <- glm.nb(cited ~ OA + coop + n_countries + old + quartile + ind + pub + uni + jap + vn, data = data_regre)

# Summary of the model
summary(nb_model)
```


Model Diagnostics


```{r}
# Fit the model (assuming data_regre is prepared)
# 1. Residual Diagnostics
# Residuals vs. Fitted
plot(nb_model, which = 1, main = "Residuals vs Fitted")

# Q-Q Plot for normality of residuals
plot(nb_model, which = 2, main = "Normal Q-Q Plot")

# Scale-Location Plot (sqrt(|standardized residuals|) vs. Fitted)
plot(nb_model, which = 3, main = "Scale-Location")

# Residuals vs. Leverage (Cook's distance)
plot(nb_model, which = 5, main = "Residuals vs Leverage")

# 2. Overdispersion Check
# Calculate dispersion parameter
dispersion <- sum(residuals(nb_model, type = "pearson")^2) / df.residual(nb_model)
print(paste("Dispersion Parameter:", round(dispersion, 3)))

# 3. Goodness-of-Fit
# Deviance and Pearson Chi-Square
deviance <- nb_model$deviance
pearson_chi <- sum(residuals(nb_model, type = "pearson")^2)
print(paste("Deviance:", round(deviance, 3)))
print(paste("Pearson Chi-Square:", round(pearson_chi, 3)))

# 4. Influence Measures
# Cook's distance
cooksd <- cooks.distance(nb_model)
influential <- cooks.distance(nb_model) > 4 / length(cooksd)  # Threshold: 4/n
print(paste("Number of Influential Points:", sum(influential)))

# Plot Cook's distance
plot(cooksd, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4 / length(cooksd), col = "red")

# 5. Summary of Model Fit
summary(nb_model)
```


quartle as iv


```{r}
library(MASS)

# Prepare data
data_regre <- data_regre |>
  mutate(quartile = factor(quartile, levels = c("Q4", "Q3", "Q2", "Q1"), ordered = TRUE)) |>
  na.omit(quartile, OA, coop, n_countries, old, ind, pub, uni, jap, vn)

# Fit ordinal logistic regression
ord_model <- polr(quartile ~ OA + coop + n_countries + old + ind + pub + uni + jap + vn, 
                  data = data_regre, Hess = TRUE)

# Summary
summary(ord_model)

# Odds ratios
exp(coef(ord_model))
```





